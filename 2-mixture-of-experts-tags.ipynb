{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 0.1: Dense MoE\n",
    "\n",
    "1. `MoE` loop each `Expert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.877403 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.655473 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_features = 7070\n",
    "n_classes = 5\n",
    "X, y = make_classification(n_samples=69, n_features=n_features, \n",
    "                           n_informative=10, n_classes=n_classes, \n",
    "                           weights=[39/69, 7/69, 7/69, 10/69, 6/69], random_state=42)\n",
    "\n",
    "# Step 2: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                  stratify=y, random_state=42)\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                              torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                            torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the Expert Network\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      self.fc1 = nn.Linear(input_dim, 50)\n",
    "      self.fc2 = nn.Linear(50, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = torch.relu(self.fc1(x))\n",
    "      return self.fc2(x)\n",
    "\n",
    "# Define the Gating Network\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, num_experts]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      return self.fc(x) # Softmax to get probabilities for each expert\n",
    "\n",
    "# Define the MoE Model\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_experts: int):\n",
    "      super().__init__()\n",
    "      self.num_experts = num_experts\n",
    "      \n",
    "      # Create the experts\n",
    "      self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "      # Create the router\n",
    "      self.router = Router(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, output_dim]\n",
    "    # Note: The gradients are computed \n",
    "    # based on how much each expert's output contribution to the loss.\n",
    "    # If the an expert is used only once for a specific sample,\n",
    "    # it will receive a gradient based only on that single sample's contribution\n",
    "    # to the overall loss.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      # Get the output from topk experts\n",
    "      expert_outputs = torch.stack([self.experts[i](x) for i in range(self.num_experts)], dim=0) # [num_experts, batch_size, output_dim]\n",
    "\n",
    "      # Get router logts\n",
    "      router_logits: torch.Tensor = self.router(x) # [batch_size, num_experts]\n",
    "\n",
    "      # Weighted sum of expert outputs\n",
    "      weighted_outputs = expert_outputs * router_logits.permute(1,0).unsqueeze(-1)\n",
    "\n",
    "      return weighted_outputs.sum(dim=0)\n",
    "\n",
    "# Training the MoE model\n",
    "def train_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Compute predictions and loss\n",
    "    pred = model(X)\n",
    "    loss: torch.Tensor = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  loss, current = loss.item(), batch * batch_size + len(X)\n",
    "  print(f\"loss: {loss:>7f} [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "# Evaluate the MoE model\n",
    "def test_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      pred: torch.Tensor = model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).sum().item()\n",
    "  \n",
    "  correct /= size\n",
    "  test_loss /= num_batches\n",
    "  print(f'Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "\n",
    "# Instantiate the MoE model\n",
    "model = MoE(input_dim=n_features, output_dim=n_classes, num_experts=4)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_loop(train_loader, model, loss_fn, optimizer) # Train the model\n",
    "  test_loop(val_loader, model, loss_fn) # Evaluate the model on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 0.2: Naive Sparse MoE\n",
    "\n",
    "1. `MoE` loop each `Expert`.\n",
    "2. `Expert` receive `x` with dimensions `[batch_size, input_dim]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.983201 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.732244 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_features = 7070\n",
    "n_classes = 5\n",
    "X, y = make_classification(n_samples=69, n_features=n_features, \n",
    "                           n_informative=10, n_classes=n_classes, \n",
    "                           weights=[39/69, 7/69, 7/69, 10/69, 6/69], random_state=42)\n",
    "\n",
    "# Step 2: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                  stratify=y, random_state=42)\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                              torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                            torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the Expert Network\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      self.fc1 = nn.Linear(input_dim, 50)\n",
    "      self.fc2 = nn.Linear(50, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = torch.relu(self.fc1(x))\n",
    "      return self.fc2(x)\n",
    "\n",
    "# Define the Gating Network\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, num_experts]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      return self.fc(x) # Softmax to get probabilities for each expert\n",
    "\n",
    "# Define the MoE Model\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_experts: int, top_k: int):\n",
    "      super().__init__()\n",
    "      self.num_experts = num_experts\n",
    "      self.top_k = top_k\n",
    "      \n",
    "      # Create the experts\n",
    "      self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "      # Create the router\n",
    "      self.router = Router(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, output_dim]\n",
    "    # Note: The gradients are computed \n",
    "    # based on how much each expert's output contribution to the loss.\n",
    "    # If the an expert is used only once for a specific sample,\n",
    "    # it will receive a gradient based only on that single sample's contribution\n",
    "    # to the overall loss.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      # Get the output from topk experts\n",
    "      expert_outputs = torch.stack([self.experts[i](x) for i in range(self.num_experts)], dim=0) # [num_experts, batch_size, output_dim]\n",
    "\n",
    "      # Get router logts\n",
    "      router_logits: torch.Tensor = self.router(x) # [batch_size, num_experts]\n",
    "      # Get the top-k expert indices for each sample in the batch\n",
    "      topk_values, topk_indices = torch.topk(router_logits, self.top_k, dim=-1) # [batch_size, top_k]\n",
    "      # Select the outputs of the top-k experts\n",
    "      selected_expert_outputs = torch.gather(expert_outputs, 0, topk_indices.unsqueeze(2).expand(-1, -1, expert_outputs.size(-1)))\n",
    "\n",
    "      # Weighted sum of expert outputs\n",
    "      weighted_outputs = selected_expert_outputs * topk_values.unsqueeze(-1)\n",
    "\n",
    "      return weighted_outputs.sum(dim=1)\n",
    "\n",
    "# Training the MoE model\n",
    "def train_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Compute predictions and loss\n",
    "    pred = model(X)\n",
    "    loss: torch.Tensor = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  loss, current = loss.item(), batch * batch_size + len(X)\n",
    "  print(f\"loss: {loss:>7f} [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "# Evaluate the MoE model\n",
    "def test_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      pred: torch.Tensor = model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).sum().item()\n",
    "  \n",
    "  correct /= size\n",
    "  test_loss /= num_batches\n",
    "  print(f'Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "\n",
    "# Instantiate the MoE model\n",
    "model = MoE(input_dim=n_features, output_dim=n_classes, num_experts=4, top_k=2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_loop(train_loader, model, loss_fn, optimizer) # Train the model\n",
    "  test_loop(val_loader, model, loss_fn) # Evaluate the model on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 0.3: Sparse MoE\n",
    "\n",
    "1. `MoE` loop each `Expert`.\n",
    "2. `Expert` receives `expert_input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.827578 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.673539 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_features = 7070\n",
    "n_classes = 5\n",
    "X, y = make_classification(n_samples=69, n_features=n_features, \n",
    "                           n_informative=10, n_classes=n_classes, \n",
    "                           weights=[39/69, 7/69, 7/69, 10/69, 6/69], random_state=42)\n",
    "\n",
    "# Step 2: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                  stratify=y, random_state=42)\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                              torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                            torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the Expert Network\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      self.fc1 = nn.Linear(input_dim, 50)\n",
    "      self.fc2 = nn.Linear(50, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = torch.relu(self.fc1(x))\n",
    "      return self.fc2(x)\n",
    "\n",
    "# Define the Gating Network\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, num_experts]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      return self.fc(x) # Softmax to get probabilities for each expert\n",
    "\n",
    "# Define the MoE Model\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_experts: int, top_k: int):\n",
    "      super().__init__()\n",
    "      self.output_dim = output_dim\n",
    "      self.num_experts = num_experts\n",
    "      self.top_k = top_k\n",
    "      \n",
    "      # Create the experts\n",
    "      self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "      # Create the router\n",
    "      self.router = Router(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, output_dim]\n",
    "    # Note: The gradients are computed \n",
    "    # based on how much each expert's output contribution to the loss.\n",
    "    # If the an expert is used only once for a specific sample,\n",
    "    # it will receive a gradient based only on that single sample's contribution\n",
    "    # to the overall loss.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      # Get router logts\n",
    "      router_logits: torch.Tensor = self.router(x) # [batch_size, num_experts]\n",
    "\n",
    "      # Get the top-k expert indices for each sample in the batch\n",
    "      topk_logits, topk_indices = torch.topk(router_logits, self.top_k, dim=-1) # [batch_size, top_k]\n",
    "\n",
    "      weighted_outputs = torch.zeros(x.size(0), self.output_dim)\n",
    "      for i, expert in enumerate(self.experts):\n",
    "        expert_mask = (topk_indices == i).any(dim=-1) # [top_k]\n",
    "\n",
    "        if expert_mask.any():\n",
    "          expert_input = x[expert_mask]\n",
    "          expert_output = expert(expert_input)\n",
    "\n",
    "          expert_logits = topk_logits[topk_indices == i]\n",
    "          weighted_output = expert_output * expert_logits.unsqueeze(-1)\n",
    "\n",
    "          weighted_outputs[expert_mask] += weighted_output\n",
    "\n",
    "      return weighted_outputs\n",
    "\n",
    "# Training the MoE model\n",
    "def train_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Compute predictions and loss\n",
    "    pred = model(X)\n",
    "    loss: torch.Tensor = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  loss, current = loss.item(), batch * batch_size + len(X)\n",
    "  print(f\"loss: {loss:>7f} [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "# Evaluate the MoE model\n",
    "def test_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      pred: torch.Tensor = model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).sum().item()\n",
    "  \n",
    "  correct /= size\n",
    "  test_loss /= num_batches\n",
    "  print(f'Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "\n",
    "# Instantiate the MoE model\n",
    "model = MoE(input_dim=n_features, output_dim=n_classes, num_experts=4, top_k=2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_loop(train_loader, model, loss_fn, optimizer) # Train the model\n",
    "  test_loop(val_loader, model, loss_fn) # Evaluate the model on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 0.4: Sparse MoE with Noise\n",
    "\n",
    "1. `MoE` loop each `Expert`.\n",
    "2. `Expert` receives `expert_input`.\n",
    "3. `Router` add noise to `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.051296 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.715019 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 4.368128 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.762916 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.000396 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.712291 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.174237 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.718284 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 6.014785 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.758571 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.928673 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 1.751489 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.067877 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.744448 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.051381 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.722980 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.715868 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.716984 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.242076 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 1.751660 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.013824 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.683523 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 10.946438 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.753926 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.035303 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 1.691884 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.016299 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 1.670499 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.006390 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.649755 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.048274 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.690840 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.000278 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.647871 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.029529 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.696328 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.012237 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.647148 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.001265 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.725710 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.005232 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.640588 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.009589 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.671571 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.000022 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.697205 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.000211 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.648421 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.000041 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.627134 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.000110 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.692922 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.002544 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.723749 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.022805 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.659995 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.001301 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.680258 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.062953 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.633218 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.000367 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.645852 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.003154 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.664977 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.050695 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.618101 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.000997 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.635387 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.039527 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.656364 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.000398 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.652491 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.001108 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.648847 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.000577 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.670520 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.000151 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.690241 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.026842 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.669599 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.000054 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.646031 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.000154 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.748528 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.000302 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.750213 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.017200 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.643770 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.000082 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.696595 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.000689 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.653239 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.000107 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.708083 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.000080 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.666243 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.006385 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.665196 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.000163 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.675508 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.000311 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.673127 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.000454 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.733627 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.028764 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.739654 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.000177 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.688061 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.000023 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.645544 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.000012 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.653533 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.000219 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.685180 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.003279 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.739439 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.000592 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.744155 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.000078 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.643242 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 1.610461 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.693305 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.000384 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.674716 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.001358 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.670399 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.000406 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.661853 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.000057 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.706747 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.001735 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.681131 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.000423 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.698831 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.000115 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.674210 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.020647 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.699561 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.000105 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.675983 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.000562 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.684356 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.007531 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.720077 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.000036 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.718946 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.000231 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.670511 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.000194 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 38.1%, Avg loss: 1.692077 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.002847 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.795257 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.000305 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.647643 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.001687 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.700755 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.000033 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.801052 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.000253 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.660164 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.000341 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.703823 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.000495 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.803430 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.000012 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.652399 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.000077 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.663506 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.000088 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.678928 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.000659 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.663164 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.000029 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.674338 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.001079 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.678775 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.002095 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.816327 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.001512 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.686228 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.000000 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.724404 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.000118 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.693370 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.000426 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.795870 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.000063 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.667612 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.000064 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.800854 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.000185 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.685817 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.000233 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.688472 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.000881 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.672413 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.000027 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.703133 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.000000 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 42.9%, Avg loss: 1.840720 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_features = 7070\n",
    "n_classes = 5\n",
    "X, y = make_classification(n_samples=69, n_features=n_features, \n",
    "                           n_informative=10, n_classes=n_classes, \n",
    "                           weights=[39/69, 7/69, 7/69, 10/69, 6/69], random_state=42)\n",
    "\n",
    "# Step 2: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                  stratify=y, random_state=42)\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                              torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                            torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the Expert Network\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      self.fc1 = nn.Linear(input_dim, 5)\n",
    "      self.fc2 = nn.Linear(5, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = torch.relu(self.fc1(x))\n",
    "      return self.fc2(x)\n",
    "\n",
    "# Define the Gating Network\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, num_experts]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = self.fc(x)\n",
    "      if self.training:\n",
    "        x += torch.randn_like(x) # Add noise to the logits\n",
    "      return x\n",
    "\n",
    "# Define the MoE Model\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_experts: int, top_k: int):\n",
    "      super().__init__()\n",
    "      self.output_dim = output_dim\n",
    "      self.num_experts = num_experts\n",
    "      self.top_k = top_k\n",
    "      \n",
    "      # Create the experts\n",
    "      self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "      # Create the router\n",
    "      self.router = Router(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, output_dim]\n",
    "    # Note: The gradients are computed \n",
    "    # based on how much each expert's output contribution to the loss.\n",
    "    # If the an expert is used only once for a specific sample,\n",
    "    # it will receive a gradient based only on that single sample's contribution\n",
    "    # to the overall loss.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      # Get router logts\n",
    "      router_logits: torch.Tensor = self.router(x) # [batch_size, num_experts]\n",
    "\n",
    "      # Get the top-k expert indices for each sample in the batch\n",
    "      topk_logits, topk_indices = torch.topk(router_logits, self.top_k, dim=-1) # [batch_size, top_k]\n",
    "\n",
    "      weighted_outputs = torch.zeros(x.size(0), self.output_dim)\n",
    "      for i, expert in enumerate(self.experts):\n",
    "        expert_mask = (topk_indices == i).any(dim=-1) # [top_k]\n",
    "\n",
    "        if expert_mask.any():\n",
    "          expert_input = x[expert_mask]\n",
    "          expert_output = expert(expert_input)\n",
    "\n",
    "          expert_logits = topk_logits[topk_indices == i]\n",
    "          weighted_output = expert_output * expert_logits.unsqueeze(-1)\n",
    "\n",
    "          weighted_outputs[expert_mask] += weighted_output\n",
    "\n",
    "      return weighted_outputs\n",
    "\n",
    "# Training the MoE model\n",
    "def train_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Compute predictions and loss\n",
    "    pred = model(X)\n",
    "    loss: torch.Tensor = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  loss, current = loss.item(), batch * batch_size + len(X)\n",
    "  print(f\"loss: {loss:>7f} [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "# Evaluate the MoE model\n",
    "def test_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      pred = model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).sum().item()\n",
    "  \n",
    "  correct /= size\n",
    "  test_loss /= num_batches\n",
    "  print(f'Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "\n",
    "# Instantiate the MoE model\n",
    "model = MoE(input_dim=n_features, output_dim=n_classes, num_experts=4, top_k=2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_loop(train_loader, model, loss_fn, optimizer) # Train the model\n",
    "  test_loop(val_loader, model, loss_fn) # Evaluate the model on the validation set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
