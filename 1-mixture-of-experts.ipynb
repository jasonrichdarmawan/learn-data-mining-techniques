{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.sparse_adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "seed = np.random.randint(10000)\n",
    "print(\"Seed: %d\" % seed)\n",
    "# seed = 835\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the microarray data\n",
    "X = pd.read_csv(\"./datasets/pp5i_train.gr.csv\").set_index(\"SNO\").rename_axis(None, axis=0)\n",
    "mapping = {'MED': 0, 'RHB': 1, 'EPD': 2, 'MGL': 3, 'JPA': 4}\n",
    "y = pd.read_csv(\"./datasets/pp5i_train_class.txt\").set_index(X.T.index).apply(lambda x: mapping[x['Class']], axis=1)\n",
    "\n",
    "# Step 2: Clipping the data \n",
    "# because the datasets was processed with MAS-4 software which generate negative values,\n",
    "# and values below 100 are less reproducible and above 16,000 tend to have non-linear correlation with the actual gene expression.\n",
    "# For dataset with more noise, lower bound of 100 is used.\n",
    "X_clipped = X.copy().clip(20, 16000)\n",
    "\n",
    "# Step 3: Filter genes based on maximum-to-minimum ratio\n",
    "# because many genes are not vary enough to be informative.\n",
    "X_filtered = X_clipped.copy()\n",
    "ratio = X_filtered.max(axis=1) / X_filtered.min(axis=1)\n",
    "X_filtered = X_filtered[ratio > 2]\n",
    "\n",
    "# Step 4: Transforming the data with log2 \n",
    "# because the assumption is that\n",
    "# genes with high values tend to have a higher variance\n",
    "# while genes with low values tend to have lower variance.\n",
    "# This makes it difficult to compare genes \n",
    "# because the variance depends on the values level.\n",
    "# Log2 makes the variance uniform across different values level.\n",
    "X_log2 = X_filtered.copy().astype(float).apply(lambda x: np.log2(x + 1))\n",
    "\n",
    "# Step 5: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered.T, y, test_size=0.5,\n",
    "                                                    stratify=y, random_state=42)\n",
    "\n",
    "# Step 6: Standardize the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "# Step 7: Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_scaled.values, dtype=torch.float32), \n",
    "                              torch.tensor(y_train.values, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_test_scaled.values, dtype=torch.float32),\n",
    "                            torch.tensor(y_test.values, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the Expert Network\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, output_dim)\n",
    "      self.dropout = nn.Dropout(0.7)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = torch.relu(self.fc(x))\n",
    "      # Apply dropout\n",
    "      # to prevent the model from overfitting\n",
    "      x = self.dropout(x)\n",
    "      return x\n",
    "\n",
    "# Define the Gating Network\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, num_experts]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = self.fc(x)\n",
    "      if self.training:\n",
    "        # Add noise to the logits drawn from a normal distribution with mean 0 and variance 0.1\n",
    "        # to encourage the model to use all the experts.\n",
    "        x += torch.randn_like(x) * (0.1**0.5) \n",
    "      return x\n",
    "\n",
    "def topk(router_logits: torch.Tensor, top_k: int, dim: int, expert_capacity: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "  \"\"\"\n",
    "  router_logits: [batch_size, num_experts]\n",
    "\n",
    "  torch.topk with `expert_capacity` constraint.\n",
    "  Suppose each expert can be used at most 2 times in a batch and \n",
    "  torch.topk use an `Expert` 3 times in a batch.\n",
    "  We will replace the 3rd usage of the expert with the next best expert for the sample.\n",
    "  \"\"\"\n",
    "  batch_size = router_logits.size(0)\n",
    "  num_experts = router_logits.size(1)\n",
    "  \n",
    "  sorted_indices = torch.argsort(router_logits, dim=dim, descending=True)\n",
    "  topk_logits, topk_indices = torch.topk(router_logits, top_k, dim=dim) # [batch_size, top_k]\n",
    "  new_logits, new_indices = topk_logits.clone(), topk_indices.clone()\n",
    "\n",
    "  expert_assigned = torch.zeros(num_experts)\n",
    "  for i in range(batch_size):\n",
    "    if (expert_assigned == expert_capacity).sum() == num_experts: # expert_capacity is full\n",
    "      new_logits[i] = 0\n",
    "      new_indices[i] = -1\n",
    "      continue\n",
    "\n",
    "    for j in range(top_k):\n",
    "      expert_index = sorted_indices[i, j]\n",
    "      if expert_assigned[expert_index] < expert_capacity:\n",
    "        expert_assigned[expert_index] += 1\n",
    "        continue\n",
    "\n",
    "      for next_expert_index in sorted_indices[i, j:]:\n",
    "        if expert_assigned[next_expert_index] == expert_capacity: # expert_capacity is full\n",
    "          continue\n",
    "\n",
    "        if next_expert_index in new_indices[i]: # prevent duplicate\n",
    "          continue\n",
    "\n",
    "        new_logits[i, j] = router_logits[i, next_expert_index]\n",
    "        new_indices[i, j] = next_expert_index\n",
    "        expert_assigned[next_expert_index] += 1\n",
    "        break\n",
    "  \n",
    "  return new_logits, new_indices\n",
    "\n",
    "# Define the MoE Model\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_experts: int, top_k: int):\n",
    "      super().__init__()\n",
    "      self.input_dim = input_dim\n",
    "      self.output_dim = output_dim\n",
    "      self.num_experts = num_experts\n",
    "      self.top_k = top_k\n",
    "      \n",
    "      # Create the experts\n",
    "      self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "      # Create the router\n",
    "      self.router = Router(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, output_dim]\n",
    "    # Note: The gradients are computed \n",
    "    # based on how much each expert's output contribution to the loss.\n",
    "    # If the an expert is used only once for a specific sample,\n",
    "    # it will receive a gradient based only on that single sample's contribution\n",
    "    # to the overall loss.\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "      batch_size = x.size(0)\n",
    "\n",
    "      # Get router logts\n",
    "      router_logits: torch.Tensor = self.router(x) # [batch_size, num_experts]\n",
    "\n",
    "      # Get the top-k expert indices for each sample in the batch\n",
    "      expert_capacity = int(batch_size * self.top_k / self.num_experts)\n",
    "      if self.training:\n",
    "        topk_logits, topk_indices = topk(router_logits, self.top_k, dim=-1, expert_capacity=expert_capacity)\n",
    "      else:\n",
    "        topk_logits, topk_indices = torch.topk(router_logits, self.top_k, dim=-1)\n",
    "\n",
    "      weighted_outputs = torch.zeros(self.num_experts, batch_size, self.output_dim)\n",
    "      for i, expert in enumerate(self.experts):\n",
    "        expert_mask = (topk_indices == i).any(dim=-1) # [top_k]\n",
    "        expert_logits = topk_logits[topk_indices == i]\n",
    "\n",
    "        if expert_mask.any():\n",
    "          expert_input = x[expert_mask]\n",
    "          expert_output = expert(expert_input)\n",
    "\n",
    "          weighted_output = expert_output * expert_logits.unsqueeze(-1)\n",
    "\n",
    "          weighted_outputs[i][expert_mask] += weighted_output\n",
    "\n",
    "      return weighted_outputs, router_logits, topk_indices\n",
    "\n",
    "# [batch_size, num_experts] -> [1]\n",
    "def load_balancing_loss_fn(router_logits: torch.Tensor) -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  If the sum of the router logits is not balanced across the experts,\n",
    "  the standard deviation of the sum will be high.\n",
    "  If the sum of the router logits is balanced across the experts,\n",
    "  the standard deviation of the sum will be low.\n",
    "  \"\"\"\n",
    "  return router_logits.sum(0).std(0)\n",
    "\n",
    "# [num_experts, batch_size, output_dim], [batch_size, output_dim] -> [1]\n",
    "def contrastive_loss(weighted_outputs: torch.Tensor, topk_indices: torch.Tensor) -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  If the output of the experts is similar, the contrastive loss will be low.\n",
    "\n",
    "  Expected cases:\n",
    "  1. Within a batch, a sample may be processed by n experts,\n",
    "  the other samples may be processed by m experts.\n",
    "\n",
    "  3 experts:\n",
    "  1. $D_{KL}(p_0||p_1)$, $D_{KL}(p_1||p_0)$\n",
    "  2. $D_{KL}(p_0||p_2)$, $D_{KL}(p_2||p_0)$\n",
    "  3. $D_{KL}(p_1||p_2)$, $D_{KL}(p_2||p_1)$\n",
    "\n",
    "  \"\"\"\n",
    "  num_experts = weighted_outputs.size(0)\n",
    "  kl_div_loss = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "  \n",
    "  loss = 0\n",
    "  for i in range(num_experts):\n",
    "    for j in range(i+1, num_experts):\n",
    "      # Version A: Assumption that each expert is guaranteed to process each sample\n",
    "      # p = torch.log_softmax(weighted_outputs[i], dim=-1)\n",
    "      # q = torch.log_softmax(weighted_outputs[j], dim=-1)\n",
    "      # d_kl_pq = kl_div_loss(p, q)\n",
    "      # d_kl_qp = kl_div_loss(q, p)\n",
    "      # loss += d_kl_pq + d_kl_qp\n",
    "\n",
    "      # Version B: Assumption that each expert is not guaranteed to process each sample\n",
    "      # Initialize a mask to ignore samples that are not processed by both experts\n",
    "      mask = (topk_indices == i).any(dim=-1) & (topk_indices == j).any(dim=-1)\n",
    "      if mask.any():\n",
    "        p = torch.log_softmax(weighted_outputs[i][mask], dim=-1)\n",
    "        q = torch.log_softmax(weighted_outputs[j][mask], dim=-1)\n",
    "        d_kl_pq = kl_div_loss(p, q)\n",
    "        d_kl_qp = kl_div_loss(q, p)\n",
    "        loss += d_kl_pq + d_kl_qp\n",
    "\n",
    "  return loss\n",
    "\n",
    "class EarlyStopping:\n",
    "  \"\"\"\n",
    "  Reference:\n",
    "  https://stackoverflow.com/a/73704579/13285583\n",
    "  \"\"\"\n",
    "  def __init__(self, patience: int, min_delta: int, mode: str):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.counter = 0\n",
    "    self.mode = mode\n",
    "    if self.mode == \"min\":\n",
    "      self.best_score = float('inf')\n",
    "    elif self.mode == \"max\":\n",
    "      self.best_score = float('-inf')\n",
    "    else:\n",
    "      raise ValueError(\"mode should be 'min' or 'max'\")\n",
    "\n",
    "  def __call__(self, score: float):\n",
    "    if self.mode == \"min\":\n",
    "      if score < self.best_score:\n",
    "        self.best_score = score\n",
    "        self.counter = 0\n",
    "      elif score > (self.best_score + self.min_delta):\n",
    "        self.counter += 1\n",
    "        if self.counter >= self.patience:\n",
    "          return True\n",
    "    elif self.mode == \"max\":\n",
    "      if score > self.best_score:\n",
    "        self.best_score = score\n",
    "        self.counter = 0\n",
    "      elif score < (self.best_score - self.min_delta):\n",
    "        self.counter += 1\n",
    "        if self.counter >= self.patience:\n",
    "          return True\n",
    "    return False\n",
    "\n",
    "# Training the MoE model\n",
    "def train_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, \n",
    "               optimizer: torch.optim.Optimizer) -> torch.Tensor:\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "  avg_loss, avg_accuracy = 0, 0\n",
    "\n",
    "  model.train()\n",
    "  for _, (X, y) in enumerate(dataloader):\n",
    "    weighted_outputs, router_logits, topk_indices = model(X)\n",
    "    pred = weighted_outputs.sum(0)\n",
    "    loss: torch.Tensor = loss_fn(pred, y)\n",
    "    loss += load_balancing_loss_fn(router_logits) * 1e-2\n",
    "    loss += contrastive_loss(weighted_outputs, topk_indices) * 1e-4\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    avg_loss += loss.item()\n",
    "    avg_accuracy += (pred.argmax(-1) == y).sum().item()\n",
    "\n",
    "  # loss, current = loss.item(), batch_index * batch_size + len(X)\n",
    "  # print(f\"loss: {loss:>7f} [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "  avg_loss /= batch_size\n",
    "  avg_accuracy /= size\n",
    "\n",
    "  return avg_loss, avg_accuracy\n",
    "\n",
    "# Evaluate the MoE model\n",
    "def test_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module) -> torch.Tensor:\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "  avg_loss, avg_accuracy = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      weighted_outputs, _, topk_indices = model(X)\n",
    "      pred = weighted_outputs.sum(0)\n",
    "      loss = loss_fn(pred, y)\n",
    "      loss += contrastive_loss(weighted_outputs, topk_indices) * 1e-4\n",
    "      avg_loss += loss.item()\n",
    "      avg_accuracy += (pred.argmax(-1) == y).sum().item()\n",
    "  \n",
    "  avg_accuracy /= size\n",
    "  avg_loss /= batch_size\n",
    "  # print(f'Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "\n",
    "  return avg_loss, avg_accuracy\n",
    "\n",
    "# Instantiate the MoE model\n",
    "_, n_features = X_train.shape\n",
    "model = MoE(input_dim=n_features, output_dim=5, num_experts=3, top_k=2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "\n",
    "epochs = 20000\n",
    "train_losses, train_accuracies = np.zeros(epochs), np.zeros(epochs)\n",
    "test_losses, test_accuracies = np.zeros(epochs), np.zeros(epochs)\n",
    "best_test_accuracy = 0\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=1e-1, mode=\"max\")\n",
    "for t in range(epochs):\n",
    "  if t % 1000 == 0:\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "  train_loss, train_accuracy = train_loop(train_loader, model, loss_fn, optimizer) # Train the model\n",
    "  train_losses[t] = train_loss\n",
    "  train_accuracies[t] = train_accuracy\n",
    "  test_loss, test_accuracy = test_loop(val_loader, model, loss_fn) # Evaluate the model on the validation set\n",
    "  test_losses[t] = test_loss\n",
    "  test_accuracies[t] = test_accuracy\n",
    "\n",
    "  # Save the best model\n",
    "  if best_test_accuracy < test_accuracy:\n",
    "    torch.save(model.state_dict(), \"model_weights_4.pth\")\n",
    "    best_test_accuracy = test_accuracy\n",
    "  \n",
    "  # Early stopping\n",
    "  if early_stopping(test_accuracy):\n",
    "    train_losses, train_accuracies = train_losses[:t+1], train_accuracies[:t+1]\n",
    "    test_losses, test_accuracies = test_losses[:t+1], test_accuracies[:t+1]\n",
    "    break\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(t+1), train_losses, 'r', label='Training loss')\n",
    "plt.plot(range(t+1), test_losses, 'b', label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(t+1), train_accuracies, 'r', label='Training accuracy')\n",
    "plt.plot(range(t+1), test_accuracies, 'b', label='Test accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "display(pd.DataFrame({(\"Loss\", \"Train\"):      [train_losses[test_losses.argmin()].round(4), train_losses[test_accuracies.argmax()].round(4)],\n",
    "                      (\"Loss\", \"Test\"):       [test_losses.min().round(4), test_losses[test_accuracies.argmax()].round(4)],\n",
    "                      (\"Accuracy\", \"Train\"):  [train_accuracies[test_losses.argmin()].round(4), train_accuracies[test_accuracies.argmax()].round(4)],\n",
    "                      (\"Accuracy\", \"Test\"):   [test_accuracies[test_losses.argmin()].round(4), test_accuracies.max().round(4)],\n",
    "                      (\"Epoch\", \"\"):          [test_losses.argmin(), test_accuracies.argmax()]},\n",
    "                     index=[\"@ Lowest Test Loss\", \"@ Highest Test Accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Results\n",
    "model = MoE(input_dim=n_features, output_dim=5, num_experts=3, top_k=2)\n",
    "model.load_state_dict(torch.load('model_weights_3.pth', weights_only=True))\n",
    "\n",
    "X_combined = torch.cat([train_dataset.tensors[0], val_dataset.tensors[0]], dim=0)\n",
    "y_combined = torch.cat([train_dataset.tensors[1], val_dataset.tensors[1]], dim=0)\n",
    "combined_dataset = TensorDataset(X_combined, y_combined)\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=69, shuffle=True)\n",
    "\n",
    "test_loop(train_loader, model, loss_fn), test_loop(val_loader, model, loss_fn), test_loop(combined_loader, model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
