{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 0.1: Dense MoE\n",
    "\n",
    "1. `MoE` loop each `Expert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.928640 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.556629 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_features = 7070\n",
    "n_classes = 5\n",
    "X, y = make_classification(n_samples=69, n_features=n_features, \n",
    "                           n_informative=10, n_classes=n_classes, \n",
    "                           weights=[39/69, 7/69, 7/69, 10/69, 6/69], random_state=42)\n",
    "\n",
    "# Step 2: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                  stratify=y, random_state=42)\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                              torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                            torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the Expert Network\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      self.fc1 = nn.Linear(input_dim, 50)\n",
    "      self.fc2 = nn.Linear(50, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = torch.relu(self.fc1(x))\n",
    "      return self.fc2(x)\n",
    "\n",
    "# Define the Gating Network\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, num_experts]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      return self.fc(x) # Softmax to get probabilities for each expert\n",
    "\n",
    "# Define the MoE Model\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_experts: int):\n",
    "      super().__init__()\n",
    "      self.num_experts = num_experts\n",
    "      \n",
    "      # Create the experts\n",
    "      self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "      # Create the router\n",
    "      self.router = Router(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, output_dim]\n",
    "    # Note: The gradients are computed \n",
    "    # based on how much each expert's output contribution to the loss.\n",
    "    # If the an expert is used only once for a specific sample,\n",
    "    # it will receive a gradient based only on that single sample's contribution\n",
    "    # to the overall loss.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      # Get the output from topk experts\n",
    "      expert_outputs = torch.stack([self.experts[i](x) for i in range(self.num_experts)], dim=0) # [num_experts, batch_size, output_dim]\n",
    "\n",
    "      # Get router logts\n",
    "      router_logits: torch.Tensor = self.router(x) # [batch_size, num_experts]\n",
    "\n",
    "      # Weighted sum of expert outputs\n",
    "      weighted_outputs = expert_outputs * router_logits.permute(1,0).unsqueeze(-1)\n",
    "\n",
    "      return weighted_outputs.sum(dim=0)\n",
    "\n",
    "# Training the MoE model\n",
    "def train_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Compute predictions and loss\n",
    "    pred = model(X)\n",
    "    loss: torch.Tensor = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  loss, current = loss.item(), batch * batch_size + len(X)\n",
    "  print(f\"loss: {loss:>7f} [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "# Evaluate the MoE model\n",
    "def test_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      pred: torch.Tensor = model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).sum().item()\n",
    "  \n",
    "  correct /= size\n",
    "  test_loss /= num_batches\n",
    "  print(f'Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "\n",
    "# Instantiate the MoE model\n",
    "model = MoE(input_dim=n_features, output_dim=n_classes, num_experts=4)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_loop(train_loader, model, loss_fn, optimizer) # Train the model\n",
    "  test_loop(val_loader, model, loss_fn) # Evaluate the model on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 0.2: Naive Sparse MoE\n",
    "\n",
    "1. `MoE` loop each `Expert`.\n",
    "2. `Expert` receive `x` with dimensions `[batch_size, input_dim]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.932255 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.689739 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_features = 7070\n",
    "n_classes = 5\n",
    "X, y = make_classification(n_samples=69, n_features=n_features, \n",
    "                           n_informative=10, n_classes=n_classes, \n",
    "                           weights=[39/69, 7/69, 7/69, 10/69, 6/69], random_state=42)\n",
    "\n",
    "# Step 2: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                  stratify=y, random_state=42)\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                              torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                            torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the Expert Network\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      self.fc1 = nn.Linear(input_dim, 50)\n",
    "      self.fc2 = nn.Linear(50, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = torch.relu(self.fc1(x))\n",
    "      return self.fc2(x)\n",
    "\n",
    "# Define the Gating Network\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, num_experts]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      return self.fc(x) # Softmax to get probabilities for each expert\n",
    "\n",
    "# Define the MoE Model\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_experts: int, top_k: int):\n",
    "      super().__init__()\n",
    "      self.num_experts = num_experts\n",
    "      self.top_k = top_k\n",
    "      \n",
    "      # Create the experts\n",
    "      self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "      # Create the router\n",
    "      self.router = Router(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, output_dim]\n",
    "    # Note: The gradients are computed \n",
    "    # based on how much each expert's output contribution to the loss.\n",
    "    # If the an expert is used only once for a specific sample,\n",
    "    # it will receive a gradient based only on that single sample's contribution\n",
    "    # to the overall loss.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      # Get the output from topk experts\n",
    "      expert_outputs = torch.stack([self.experts[i](x) for i in range(self.num_experts)], dim=0) # [num_experts, batch_size, output_dim]\n",
    "\n",
    "      # Get router logts\n",
    "      router_logits: torch.Tensor = self.router(x) # [batch_size, num_experts]\n",
    "      # Get the top-k expert indices for each sample in the batch\n",
    "      topk_values, topk_indices = torch.topk(router_logits, self.top_k, dim=-1) # [batch_size, top_k]\n",
    "      # Select the outputs of the top-k experts\n",
    "      selected_expert_outputs = torch.gather(expert_outputs, 0, topk_indices.unsqueeze(2).expand(-1, -1, expert_outputs.size(-1)))\n",
    "\n",
    "      # Weighted sum of expert outputs\n",
    "      weighted_outputs = selected_expert_outputs * topk_values.unsqueeze(-1)\n",
    "\n",
    "      return weighted_outputs.sum(dim=1)\n",
    "\n",
    "# Training the MoE model\n",
    "def train_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Compute predictions and loss\n",
    "    pred = model(X)\n",
    "    loss: torch.Tensor = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  loss, current = loss.item(), batch * batch_size + len(X)\n",
    "  print(f\"loss: {loss:>7f} [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "# Evaluate the MoE model\n",
    "def test_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      pred: torch.Tensor = model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).sum().item()\n",
    "  \n",
    "  correct /= size\n",
    "  test_loss /= num_batches\n",
    "  print(f'Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "\n",
    "# Instantiate the MoE model\n",
    "model = MoE(input_dim=n_features, output_dim=n_classes, num_experts=4, top_k=2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_loop(train_loader, model, loss_fn, optimizer) # Train the model\n",
    "  test_loop(val_loader, model, loss_fn) # Evaluate the model on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 0.3: Sparse MoE\n",
    "\n",
    "1. `MoE` loop each `Expert`.\n",
    "2. `Expert` receives `expert_input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.812027 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 1.629644 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_features = 7070\n",
    "n_classes = 5\n",
    "X, y = make_classification(n_samples=69, n_features=n_features, \n",
    "                           n_informative=10, n_classes=n_classes, \n",
    "                           weights=[39/69, 7/69, 7/69, 10/69, 6/69], random_state=42)\n",
    "\n",
    "# Step 2: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                  stratify=y, random_state=42)\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                              torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                            torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the Expert Network\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      self.fc1 = nn.Linear(input_dim, 50)\n",
    "      self.fc2 = nn.Linear(50, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = torch.relu(self.fc1(x))\n",
    "      return self.fc2(x)\n",
    "\n",
    "# Define the Gating Network\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, num_experts]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      return self.fc(x) # Softmax to get probabilities for each expert\n",
    "\n",
    "# Define the MoE Model\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_experts: int, top_k: int):\n",
    "      super().__init__()\n",
    "      self.output_dim = output_dim\n",
    "      self.num_experts = num_experts\n",
    "      self.top_k = top_k\n",
    "      \n",
    "      # Create the experts\n",
    "      self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "      # Create the router\n",
    "      self.router = Router(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, output_dim]\n",
    "    # Note: The gradients are computed \n",
    "    # based on how much each expert's output contribution to the loss.\n",
    "    # If the an expert is used only once for a specific sample,\n",
    "    # it will receive a gradient based only on that single sample's contribution\n",
    "    # to the overall loss.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      # Get router logts\n",
    "      router_logits: torch.Tensor = self.router(x) # [batch_size, num_experts]\n",
    "\n",
    "      # Get the top-k expert indices for each sample in the batch\n",
    "      topk_logits, topk_indices = torch.topk(router_logits, self.top_k, dim=-1) # [batch_size, top_k]\n",
    "\n",
    "      weighted_outputs = torch.zeros(x.size(0), self.output_dim)\n",
    "      for i, expert in enumerate(self.experts):\n",
    "        expert_mask = (topk_indices == i).any(dim=-1) # [top_k]\n",
    "\n",
    "        if expert_mask.any():\n",
    "          expert_input = x[expert_mask]\n",
    "          expert_output = expert(expert_input)\n",
    "\n",
    "          expert_logits = topk_logits[topk_indices == i]\n",
    "          weighted_output = expert_output * expert_logits.unsqueeze(-1)\n",
    "\n",
    "          weighted_outputs[expert_mask] += weighted_output\n",
    "\n",
    "      return weighted_outputs\n",
    "\n",
    "# Training the MoE model\n",
    "def train_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Compute predictions and loss\n",
    "    pred = model(X)\n",
    "    loss: torch.Tensor = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  loss, current = loss.item(), batch * batch_size + len(X)\n",
    "  print(f\"loss: {loss:>7f} [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "# Evaluate the MoE model\n",
    "def test_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      pred: torch.Tensor = model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).sum().item()\n",
    "  \n",
    "  correct /= size\n",
    "  test_loss /= num_batches\n",
    "  print(f'Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "\n",
    "# Instantiate the MoE model\n",
    "model = MoE(input_dim=n_features, output_dim=n_classes, num_experts=4, top_k=2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_loop(train_loader, model, loss_fn, optimizer) # Train the model\n",
    "  test_loop(val_loader, model, loss_fn) # Evaluate the model on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 0.4: Sparse MoE with Noise\n",
    "\n",
    "1. `MoE` loop each `Expert`.\n",
    "2. `Expert` receives `expert_input`.\n",
    "3. `Router` add noise to `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.364713 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 4.8%, Avg loss: 1.797316 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.157993 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.802431 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.823367 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.744045 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 11.269528 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.763081 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.021193 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.789855 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.799904 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.717020 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.451883 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.717686 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.172014 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.716439 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.679526 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.731067 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.908116 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.758321 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.415553 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.723069 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.269351 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.736143 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.308397 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.725478 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.233114 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.708881 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.104882 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.736603 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.464360 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.686280 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.202862 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.726981 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.000247 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.685160 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.199437 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.743575 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.424755 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.749903 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.000094 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.715294 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.210865 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.723847 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.003915 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.725106 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.008136 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.676187 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.142486 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.696341 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.208858 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.741488 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.006894 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.716853 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.003579 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.695975 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.000478 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.698392 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.000007 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.703454 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.206666 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.701916 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.200025 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.737112 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.246665 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.742017 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.192433 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.740027 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.000014 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.773459 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.032116 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.705022 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.195412 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.692302 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.198061 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.707496 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.002390 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.744512 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.000095 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.731939 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.191530 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.708781 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.271338 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.713506 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.195971 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.736994 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.388595 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.681244 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.193191 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.745800 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.186520 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.729259 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.197162 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.712882 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.000009 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.723920 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.003073 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.737276 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.000140 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.717648 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.000787 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.719480 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.184347 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.765661 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.001976 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.691326 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.000928 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.757029 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.000367 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.738066 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.000007 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.723549 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.000019 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.708447 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 1.033144 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.778438 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.000015 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.737933 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.140907 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.753781 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.000002 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.766715 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.150940 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.755266 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.000237 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.793318 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 5.298560 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.723488 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 1.593091 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.768208 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.023401 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.755845 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.001751 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 4.8%, Avg loss: 1.824971 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.000054 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 4.8%, Avg loss: 1.786244 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.003894 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 4.8%, Avg loss: 1.790101 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.000006 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 4.8%, Avg loss: 1.761881 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.168006 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 4.8%, Avg loss: 1.804395 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.003000 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 9.5%, Avg loss: 1.726957 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.106885 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.735878 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.030787 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.730078 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.160455 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.793375 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.000024 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.755397 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.000000 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.744447 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.059507 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.753089 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.003224 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.731251 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.065989 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.761483 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.000235 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.707519 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.157781 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.724775 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.096697 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.706241 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.069472 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 1.768675 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.000322 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.718701 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 1.669671 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.698360 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.030662 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.707649 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.037663 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.753468 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.000194 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.709621 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.047960 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.686862 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.000347 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.653453 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.042079 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 1.666788 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.038631 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 1.657222 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.000002 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 1.711031 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 33.686726 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 1.730270 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.000004 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 1.721263 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.000000 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 1.726872 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.045669 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 23.8%, Avg loss: 1.837022 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.021929 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 1.798340 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.003126 [48/48]\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 1.771301 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_features = 7070\n",
    "n_classes = 5\n",
    "X, y = make_classification(n_samples=69, n_features=n_features, \n",
    "                           n_informative=10, n_classes=n_classes, \n",
    "                           weights=[39/69, 7/69, 7/69, 10/69, 6/69], random_state=42)\n",
    "\n",
    "# Step 2: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                  stratify=y, random_state=42)\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                              torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                            torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the Expert Network\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      self.fc1 = nn.Linear(input_dim, 5)\n",
    "      self.fc2 = nn.Linear(5, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = torch.relu(self.fc1(x))\n",
    "      return self.fc2(x)\n",
    "\n",
    "# Define the Gating Network\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, num_experts]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = self.fc(x)\n",
    "      if self.training:\n",
    "        x += torch.randn_like(x) # Add noise to the logits\n",
    "      return x\n",
    "\n",
    "# Define the MoE Model\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_experts: int, top_k: int):\n",
    "      super().__init__()\n",
    "      self.output_dim = output_dim\n",
    "      self.num_experts = num_experts\n",
    "      self.top_k = top_k\n",
    "      \n",
    "      # Create the experts\n",
    "      self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "      # Create the router\n",
    "      self.router = Router(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, output_dim]\n",
    "    # Note: The gradients are computed \n",
    "    # based on how much each expert's output contribution to the loss.\n",
    "    # If the an expert is used only once for a specific sample,\n",
    "    # it will receive a gradient based only on that single sample's contribution\n",
    "    # to the overall loss.\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      # Get router logts\n",
    "      router_logits: torch.Tensor = self.router(x) # [batch_size, num_experts]\n",
    "\n",
    "      # Get the top-k expert indices for each sample in the batch\n",
    "      topk_logits, topk_indices = torch.topk(router_logits, self.top_k, dim=-1) # [batch_size, top_k]\n",
    "\n",
    "      weighted_outputs = torch.zeros(x.size(0), self.output_dim)\n",
    "      for i, expert in enumerate(self.experts):\n",
    "        expert_mask = (topk_indices == i).any(dim=-1) # [top_k]\n",
    "\n",
    "        if expert_mask.any():\n",
    "          expert_input = x[expert_mask]\n",
    "          expert_output = expert(expert_input)\n",
    "\n",
    "          expert_logits = topk_logits[topk_indices == i]\n",
    "          weighted_output = expert_output * expert_logits.unsqueeze(-1)\n",
    "\n",
    "          weighted_outputs[expert_mask] += weighted_output\n",
    "\n",
    "      return weighted_outputs\n",
    "\n",
    "# Training the MoE model\n",
    "def train_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Compute predictions and loss\n",
    "    pred = model(X)\n",
    "    loss: torch.Tensor = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  loss, current = loss.item(), batch * batch_size + len(X)\n",
    "  print(f\"loss: {loss:>7f} [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "# Evaluate the MoE model\n",
    "def test_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      pred = model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).sum().item()\n",
    "  \n",
    "  correct /= size\n",
    "  test_loss /= num_batches\n",
    "  print(f'Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "\n",
    "# Instantiate the MoE model\n",
    "model = MoE(input_dim=n_features, output_dim=n_classes, num_experts=4, top_k=2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_loop(train_loader, model, loss_fn, optimizer) # Train the model\n",
    "  test_loop(val_loader, model, loss_fn) # Evaluate the model on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 0.5: Expert Capacity \"Prevent Overtrained Expert Train\"\n",
    "\n",
    "1. `MoE` loop each `Expert`.\n",
    "2. If the `expert_capacity` is exceeded, then the `sample`(s) are removed from the `mini-batch`.\n",
    "3. `Expert` receives `expert_input`.\n",
    "4. `Router` add noise to `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17e2e8bb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_features = 7070\n",
    "n_classes = 5\n",
    "X, y = make_classification(n_samples=69, n_features=n_features, \n",
    "                           n_informative=10, n_classes=n_classes, \n",
    "                           weights=[39/69, 7/69, 7/69, 10/69, 6/69], random_state=42)\n",
    "\n",
    "# Step 2: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    stratify=y, random_state=42)\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                              torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                            torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the Expert Network\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      return torch.relu(self.fc(x))\n",
    "\n",
    "# Define the Gating Network\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, num_experts]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = self.fc(x)\n",
    "      if self.training:\n",
    "        x += torch.randn_like(x) # Add noise to the logits\n",
    "      return x\n",
    "    \n",
    "def topk(router_logits: torch.Tensor, top_k: int, dim: int, expert_capacity: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "  batch_size = router_logits.size(0)\n",
    "  num_experts = router_logits.size(1)\n",
    "  topk_logits, topk_indices = torch.topk(router_logits, top_k, dim=dim) # [batch_size, top_k]\n",
    "  \n",
    "  sorted_indices = torch.argsort(router_logits, dim=dim, descending=True)\n",
    "  new_logits, new_indices = topk_logits.clone(), topk_indices.clone()\n",
    "\n",
    "  expert_assigned = torch.zeros(num_experts)\n",
    "  for i in range(batch_size):\n",
    "    if (expert_assigned == expert_capacity).sum() == 4:\n",
    "      break\n",
    "\n",
    "    for j in range(top_k):\n",
    "      expert_index = sorted_indices[i, j]\n",
    "      if expert_assigned[expert_index] < expert_capacity:\n",
    "        expert_assigned[expert_index] += 1\n",
    "        continue\n",
    "\n",
    "      for next_expert_index in sorted_indices[i, j:]:\n",
    "        if expert_assigned[next_expert_index] == expert_capacity:\n",
    "          continue\n",
    "\n",
    "        if next_expert_index in new_indices[i]: # prevent duplicate\n",
    "          continue\n",
    "\n",
    "        new_logits[i, j] = router_logits[i, next_expert_index]\n",
    "        new_indices[i, j] = next_expert_index\n",
    "        expert_assigned[next_expert_index] += 1\n",
    "        break\n",
    "\n",
    "  return new_logits, new_indices\n",
    "\n",
    "# Define the MoE Model\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_experts: int, top_k: int):\n",
    "      super().__init__()\n",
    "      self.output_dim = output_dim\n",
    "      self.num_experts = num_experts\n",
    "      self.top_k = top_k\n",
    "      \n",
    "      # Create the experts\n",
    "      self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "      # Create the router\n",
    "      self.router = Router(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, output_dim]\n",
    "    # Note: The gradients are computed \n",
    "    # based on how much each expert's output contribution to the loss.\n",
    "    # If the an expert is used only once for a specific sample,\n",
    "    # it will receive a gradient based only on that single sample's contribution\n",
    "    # to the overall loss.\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "      # Get router logts\n",
    "      router_logits: torch.Tensor = self.router(x) # [batch_size, num_experts]\n",
    "\n",
    "      # Get the top-k expert indices for each sample in the batch\n",
    "      expert_capacity = int(x.size(0) * self.top_k / self.num_experts)\n",
    "      topk_logits, topk_indices = torch.topk(router_logits, self.top_k, dim=-1) # [batch_size, top_k]\n",
    "\n",
    "      weighted_outputs = torch.zeros(x.size(0), self.output_dim)\n",
    "      for i, expert in enumerate(self.experts):\n",
    "        expert_mask = (topk_indices == i).any(dim=-1) # [top_k]\n",
    "\n",
    "        true_indicies = torch.nonzero(expert_mask)\n",
    "        if true_indicies.size(0) > expert_capacity:\n",
    "          expert_mask[true_indicies[expert_capacity:]] = False\n",
    "\n",
    "        if expert_mask.any():\n",
    "          expert_input = x[expert_mask]\n",
    "          expert_output = expert(expert_input)\n",
    "\n",
    "          expert_logits = topk_logits[topk_indices == i][:expert_capacity]\n",
    "\n",
    "          weighted_output = expert_output * expert_logits.unsqueeze(-1)\n",
    "\n",
    "          weighted_outputs[expert_mask] += weighted_output\n",
    "\n",
    "      return weighted_outputs, router_logits\n",
    "\n",
    "# [batch_size, num_experts] -> [1]\n",
    "def load_balancing_loss_fn(router_logits: torch.Tensor) -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  If the sum of the router logits is not balanced across the experts,\n",
    "  the standard deviation of the sum will be high.\n",
    "  If the sum of the router logits is balanced across the experts,\n",
    "  the standard deviation of the sum will be low.\n",
    "  \"\"\"\n",
    "  return router_logits.sum(0).std(0)\n",
    "\n",
    "# Training the MoE model\n",
    "def train_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, \n",
    "               optimizer: torch.optim.Optimizer) -> torch.Tensor:\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "  avg_loss, avg_accuracy = 0, 0\n",
    "\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Compute predictions and loss\n",
    "    pred, router_logits = model(X)\n",
    "    loss: torch.Tensor = loss_fn(pred, y) + load_balancing_loss_fn(router_logits) * 1e-4\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    avg_loss += loss.item()\n",
    "    avg_accuracy += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "  # loss, current = loss.item(), batch * batch_size + len(X)\n",
    "  # print(f\"loss: {loss:>7f} [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "  avg_loss /= batch_size\n",
    "  avg_accuracy /= size\n",
    "\n",
    "  return avg_loss, avg_accuracy\n",
    "\n",
    "# Evaluate the MoE model\n",
    "def test_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module) -> torch.Tensor:\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "  avg_loss, avg_accuracy = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      pred, _ = model(X)\n",
    "      avg_loss += loss_fn(pred, y).item()\n",
    "      avg_accuracy += (pred.argmax(1) == y).sum().item()\n",
    "  \n",
    "  avg_accuracy /= size\n",
    "  avg_loss /= batch_size\n",
    "  # print(f'Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "\n",
    "  return avg_loss, avg_accuracy\n",
    "\n",
    "# Instantiate the MoE model\n",
    "model = MoE(input_dim=n_features, output_dim=n_classes, num_experts=3, top_k=1)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 1\n",
    "train_losses, train_accuracies = [], []\n",
    "test_losses, test_accuracies = [], []\n",
    "for t in range(epochs):\n",
    "  # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_loss, train_accuracy = train_loop(train_loader, model, loss_fn, optimizer) # Train the model\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies.append(train_accuracy)\n",
    "  test_loss, test_accuracy = test_loop(val_loader, model, loss_fn) # Evaluate the model on the validation set\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies.append(test_accuracy)\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(epochs), train_losses, 'r', label='Training loss')\n",
    "plt.plot(range(epochs), test_losses, 'b', label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(epochs), train_accuracies, 'r', label='Training accuracy')\n",
    "plt.plot(range(epochs), test_accuracies, 'b', label='Test accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 0.6: Expert Capacity \"Naive Let Undertrained Expert Train\"\n",
    "\n",
    "1. `MoE` loop each `Expert`.\n",
    "2. If the `Expert`'s `expert_capacity` is full, then pass the `sample`(s) to the next top-(k+1) expert. If all `Expert` have their `expert_capacity` full, then the remaining `sample`(s) are removed from the `mini-batch`.\n",
    "3. `Expert` receives `expert_input`.\n",
    "4. `Router` add noise to `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17b29ddb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_features = 7070\n",
    "n_classes = 5\n",
    "X, y = make_classification(n_samples=69, n_features=n_features, \n",
    "                           n_informative=10, n_classes=n_classes, \n",
    "                           weights=[39/69, 7/69, 7/69, 10/69, 6/69], random_state=42)\n",
    "\n",
    "# Step 2: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    stratify=y, random_state=42)\n",
    "\n",
    "# Step 3: Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                              torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                            torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the Expert Network\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      return torch.relu(self.fc(x))\n",
    "\n",
    "# Define the Gating Network\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "      super().__init__()\n",
    "      self.fc = nn.Linear(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, num_experts]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "      x = self.fc(x)\n",
    "      if self.training:\n",
    "        x += torch.randn_like(x) # Add noise to the logits\n",
    "      return x\n",
    "    \n",
    "def topk(router_logits: torch.Tensor, top_k: int, dim: int, expert_capacity: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "  batch_size = router_logits.size(0)\n",
    "  num_experts = router_logits.size(1)\n",
    "  topk_logits, topk_indices = torch.topk(router_logits, top_k, dim=dim) # [batch_size, top_k]\n",
    "  \n",
    "  sorted_indices = torch.argsort(router_logits, dim=dim, descending=True)\n",
    "  new_logits, new_indices = topk_logits.clone(), topk_indices.clone()\n",
    "\n",
    "  expert_assigned = torch.zeros(num_experts)\n",
    "  for i in range(batch_size):\n",
    "    if (expert_assigned == expert_capacity).sum() == 4:\n",
    "      break\n",
    "\n",
    "    for j in range(top_k):\n",
    "      expert_index = sorted_indices[i, j]\n",
    "      if expert_assigned[expert_index] < expert_capacity:\n",
    "        expert_assigned[expert_index] += 1\n",
    "        continue\n",
    "\n",
    "      for next_expert_index in sorted_indices[i, j:]:\n",
    "        if expert_assigned[next_expert_index] == expert_capacity:\n",
    "          continue\n",
    "\n",
    "        if next_expert_index in new_indices[i]: # prevent duplicate\n",
    "          continue\n",
    "\n",
    "        new_logits[i, j] = router_logits[i, next_expert_index]\n",
    "        new_indices[i, j] = next_expert_index\n",
    "        expert_assigned[next_expert_index] += 1\n",
    "        break\n",
    "\n",
    "  return new_logits, new_indices\n",
    "\n",
    "# Define the MoE Model\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_experts: int, top_k: int):\n",
    "      super().__init__()\n",
    "      self.output_dim = output_dim\n",
    "      self.num_experts = num_experts\n",
    "      self.top_k = top_k\n",
    "      \n",
    "      # Create the experts\n",
    "      self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "      # Create the router\n",
    "      self.router = Router(input_dim, num_experts)\n",
    "    \n",
    "    # [batch_size, input_dim] -> [batch_size, output_dim]\n",
    "    # Note: The gradients are computed \n",
    "    # based on how much each expert's output contribution to the loss.\n",
    "    # If the an expert is used only once for a specific sample,\n",
    "    # it will receive a gradient based only on that single sample's contribution\n",
    "    # to the overall loss.\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "      # Get router logts\n",
    "      router_logits: torch.Tensor = self.router(x) # [batch_size, num_experts]\n",
    "\n",
    "      # Get the top-k expert indices for each sample in the batch\n",
    "      expert_capacity = int(x.size(0) * self.top_k / self.num_experts)\n",
    "      # Version 0.5: Expert Capacity \"Prevent Overtrained Expert Train\"\n",
    "      # topk_logits, topk_indices = torch.topk(router_logits, self.top_k, dim=-1) # [batch_size, top_k]\n",
    "      # Version 0.6: Expert Capacity \"Naive Let Undertrained Expert Train\"\n",
    "      topk_logits, topk_indices = topk(router_logits, self.top_k, dim=-1, expert_capacity=expert_capacity)\n",
    "\n",
    "      weighted_outputs = torch.zeros(x.size(0), self.output_dim)\n",
    "      for i, expert in enumerate(self.experts):\n",
    "        expert_mask = (topk_indices == i).any(dim=-1) # [top_k]\n",
    "\n",
    "        true_indicies = torch.nonzero(expert_mask)\n",
    "        if true_indicies.size(0) > expert_capacity:\n",
    "          expert_mask[true_indicies[expert_capacity:]] = False\n",
    "\n",
    "        if expert_mask.any():\n",
    "          expert_input = x[expert_mask]\n",
    "          expert_output = expert(expert_input)\n",
    "\n",
    "          expert_logits = topk_logits[topk_indices == i][:expert_capacity]\n",
    "\n",
    "          weighted_output = expert_output * expert_logits.unsqueeze(-1)\n",
    "\n",
    "          weighted_outputs[expert_mask] += weighted_output\n",
    "\n",
    "      return weighted_outputs, router_logits\n",
    "\n",
    "# [batch_size, num_experts] -> [1]\n",
    "def load_balancing_loss_fn(router_logits: torch.Tensor) -> torch.Tensor:\n",
    "  \"\"\"\n",
    "  If the sum of the router logits is not balanced across the experts,\n",
    "  the standard deviation of the sum will be high.\n",
    "  If the sum of the router logits is balanced across the experts,\n",
    "  the standard deviation of the sum will be low.\n",
    "  \"\"\"\n",
    "  return router_logits.sum(0).std(0)\n",
    "\n",
    "# Training the MoE model\n",
    "def train_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, \n",
    "               optimizer: torch.optim.Optimizer) -> torch.Tensor:\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "  avg_loss, avg_accuracy = 0, 0\n",
    "\n",
    "  model.train()\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    # Compute predictions and loss\n",
    "    pred, router_logits = model(X)\n",
    "    loss: torch.Tensor = loss_fn(pred, y) + load_balancing_loss_fn(router_logits) * 1e-4\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    avg_loss += loss.item()\n",
    "    avg_accuracy += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "  # loss, current = loss.item(), batch * batch_size + len(X)\n",
    "  # print(f\"loss: {loss:>7f} [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "  avg_loss /= batch_size\n",
    "  avg_accuracy /= size\n",
    "\n",
    "  return avg_loss, avg_accuracy\n",
    "\n",
    "# Evaluate the MoE model\n",
    "def test_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module) -> torch.Tensor:\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  batch_size = dataloader.batch_size\n",
    "  avg_loss, avg_accuracy = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      pred, _ = model(X)\n",
    "      avg_loss += loss_fn(pred, y).item()\n",
    "      avg_accuracy += (pred.argmax(1) == y).sum().item()\n",
    "  \n",
    "  avg_accuracy /= size\n",
    "  avg_loss /= batch_size\n",
    "  # print(f'Test Error: \\n Accuracy: {100 * correct:>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "\n",
    "  return avg_loss, avg_accuracy\n",
    "\n",
    "# Instantiate the MoE model\n",
    "model = MoE(input_dim=n_features, output_dim=n_classes, num_experts=3, top_k=1)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 1\n",
    "train_losses, train_accuracies = [], []\n",
    "test_losses, test_accuracies = [], []\n",
    "for t in range(epochs):\n",
    "  # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "  train_loss, train_accuracy = train_loop(train_loader, model, loss_fn, optimizer) # Train the model\n",
    "  train_losses.append(train_loss)\n",
    "  train_accuracies.append(train_accuracy)\n",
    "  test_loss, test_accuracy = test_loop(val_loader, model, loss_fn) # Evaluate the model on the validation set\n",
    "  test_losses.append(test_loss)\n",
    "  test_accuracies.append(test_accuracy)\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(epochs), train_losses, 'r', label='Training loss')\n",
    "plt.plot(range(epochs), test_losses, 'b', label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(epochs), train_accuracies, 'r', label='Training accuracy')\n",
    "plt.plot(range(epochs), test_accuracies, 'b', label='Test accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
